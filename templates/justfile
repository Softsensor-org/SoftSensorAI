# Justfile - Command runner for project tasks
# https://github.com/casey/just

# Default recipe to display help information
default:
  @just --list --unsorted

# Variables
export NODE_ENV := env_var_or_default("NODE_ENV", "development")
export CI := env_var_or_default("CI", "false")

# ============================================================================
# AI Commands
# ============================================================================

# AI review of local changes (CLI-first, no secrets)
review-local:
  #!/usr/bin/env bash
  set -euo pipefail
  echo "Running AI review on local changes..."
  diff=$(git diff --cached)
  if [ -z "$diff" ]; then
    echo "No staged changes. Checking working tree..."
    diff=$(git diff)
  fi
  if [ -z "$diff" ]; then
    echo "No changes to review"
    exit 0
  fi

  # Try each CLI until one works
  for cli in claude codex gemini grok; do
    if command -v $cli >/dev/null 2>&1; then
      echo "Using $cli for review..."
      echo "$diff" | $cli --system-prompt system/active.md \
        "Review this diff for security, performance, and bugs. Be concise."
      exit 0
    fi
  done
  echo "No AI CLI found. Install claude, codex, gemini, or grok."

# Generate tickets from codebase (outputs JSON and CSV)
tickets:
  #!/usr/bin/env bash
  set -euo pipefail
  echo "Generating tickets from codebase..."

  # Create output directory
  mkdir -p artifacts

  # Try each CLI until one works
  for cli in claude codex gemini grok; do
    if command -v $cli >/dev/null 2>&1; then
      echo "Using $cli..."
      $cli --system-prompt .claude/commands/tickets-from-code.md \
        "analyze entire codebase" > artifacts/tickets_raw.txt

      # Extract JSON and convert to CSV
      awk '/^{/,0' artifacts/tickets_raw.txt > artifacts/tickets.json || true
      if [ -s artifacts/tickets.json ] && jq -e . artifacts/tickets.json >/dev/null 2>&1; then
        jq -r '.tickets[] | [.id, .title, .priority, .effort] | @csv' \
          artifacts/tickets.json > artifacts/backlog.csv
        echo "✓ Generated tickets:"
        echo "  JSON: artifacts/tickets.json"
        echo "  CSV: artifacts/backlog.csv"
      else
        echo "Generated output but not valid JSON"
      fi
      exit 0
    fi
  done
  echo "No AI CLI found. Install claude, codex, gemini, or grok."

# ============================================================================
# Development
# ============================================================================

# Install all dependencies
install:
  #!/usr/bin/env bash
  set -euo pipefail
  echo "Installing dependencies..."
  if [ -f "package.json" ]; then
    if [ -f "pnpm-lock.yaml" ]; then
      pnpm install
    elif [ -f "yarn.lock" ]; then
      yarn install
    else
      npm install
    fi
  fi
  if [ -f "requirements.txt" ]; then
    pip install -r requirements.txt
  elif [ -f "pyproject.toml" ]; then
    pip install -e .
  fi
  if [ -f "go.mod" ]; then
    go mod download
  fi
  if [ -f "Cargo.toml" ]; then
    cargo fetch
  fi
  echo "✓ Dependencies installed"

# Start development server
dev:
  #!/usr/bin/env bash
  set -euo pipefail
  if [ -f "package.json" ]; then
    npm run dev || npm start
  elif [ -f "manage.py" ]; then
    python manage.py runserver
  elif [ -f "main.go" ]; then
    go run .
  elif [ -f "Cargo.toml" ]; then
    cargo run
  else
    echo "No dev server configuration found"
    exit 1
  fi

# Run the application
run *ARGS:
  #!/usr/bin/env bash
  set -euo pipefail
  if [ -f "package.json" ]; then
    npm start {{ARGS}}
  elif [ -f "main.py" ]; then
    python main.py {{ARGS}}
  elif [ -f "main.go" ]; then
    go run . {{ARGS}}
  elif [ -f "Cargo.toml" ]; then
    cargo run {{ARGS}}
  else
    echo "No run configuration found"
    exit 1
  fi

# ============================================================================
# Testing
# ============================================================================

# Run all tests
test:
  @just test-unit
  @just test-integration || true
  @just test-e2e || true

# Run unit tests
test-unit:
  #!/usr/bin/env bash
  set -euo pipefail
  if [ -f "package.json" ] && grep -q '"test"' package.json; then
    npm test
  elif [ -f "pytest.ini" ] || [ -f "setup.cfg" ] || [ -f "pyproject.toml" ]; then
    pytest tests/unit -v
  elif [ -f "go.mod" ]; then
    go test ./...
  elif [ -f "Cargo.toml" ]; then
    cargo test
  else
    echo "No test configuration found"
  fi

# Run integration tests
test-integration:
  #!/usr/bin/env bash
  set -euo pipefail
  if [ -d "tests/integration" ]; then
    pytest tests/integration -v || npm run test:integration || go test ./tests/integration/...
  else
    echo "No integration tests found"
  fi

# Run e2e tests
test-e2e:
  #!/usr/bin/env bash
  set -euo pipefail
  if [ -f "playwright.config.ts" ] || [ -f "playwright.config.js" ]; then
    npx playwright test
  elif [ -f "cypress.config.ts" ] || [ -f "cypress.config.js" ]; then
    npx cypress run
  elif [ -d "tests/e2e" ]; then
    pytest tests/e2e -v
  else
    echo "No e2e tests found"
  fi

# Run tests with coverage
coverage:
  #!/usr/bin/env bash
  set -euo pipefail
  if [ -f "package.json" ]; then
    npm run test:coverage || npx jest --coverage
  elif command -v pytest >/dev/null; then
    pytest --cov=. --cov-report=html --cov-report=term
  elif [ -f "go.mod" ]; then
    go test -coverprofile=coverage.out ./...
    go tool cover -html=coverage.out
  else
    echo "No coverage configuration found"
  fi

# ============================================================================
# Code Quality
# ============================================================================

# Run linter
lint:
  #!/usr/bin/env bash
  set -euo pipefail
  echo "Running linters..."
  if [ -f "package.json" ]; then
    npm run lint || npx eslint . || true
  fi
  if [ -f "pyproject.toml" ] || [ -f "setup.cfg" ]; then
    ruff check . || flake8 . || pylint . || true
  fi
  if [ -f "go.mod" ]; then
    golangci-lint run || go vet ./... || true
  fi
  if [ -f "Cargo.toml" ]; then
    cargo clippy || true
  fi
  echo "✓ Linting complete"

# Format code
format:
  #!/usr/bin/env bash
  set -euo pipefail
  echo "Formatting code..."
  if [ -f "package.json" ]; then
    npm run format || npx prettier --write . || true
  fi
  if command -v black >/dev/null; then
    black . || true
  elif command -v ruff >/dev/null; then
    ruff format . || true
  fi
  if [ -f "go.mod" ]; then
    go fmt ./... || true
  fi
  if [ -f "Cargo.toml" ]; then
    cargo fmt || true
  fi
  echo "✓ Formatting complete"

# Type check
typecheck:
  #!/usr/bin/env bash
  set -euo pipefail
  if [ -f "tsconfig.json" ]; then
    npx tsc --noEmit
  elif command -v mypy >/dev/null; then
    mypy .
  else
    echo "No type checking configuration found"
  fi

# Run security scan
security:
  #!/usr/bin/env bash
  set -euo pipefail
  echo "Running security scans..."
  if command -v trivy >/dev/null; then
    trivy fs .
  fi
  if command -v semgrep >/dev/null; then
    semgrep --config=auto .
  fi
  if command -v gitleaks >/dev/null; then
    gitleaks detect --source . -v
  fi
  echo "✓ Security scan complete"

# ============================================================================
# Build & Deploy
# ============================================================================

# Build the project
build:
  #!/usr/bin/env bash
  set -euo pipefail
  echo "Building project..."
  if [ -f "package.json" ]; then
    npm run build
  elif [ -f "Makefile" ]; then
    make build
  elif [ -f "go.mod" ]; then
    go build -o bin/app .
  elif [ -f "Cargo.toml" ]; then
    cargo build --release
  elif [ -f "Dockerfile" ]; then
    docker build -t app:latest .
  else
    echo "No build configuration found"
  fi
  echo "✓ Build complete"

# Build Docker image
docker-build TAG="latest":
  docker build -t app:{{TAG}} .
  echo "✓ Docker image built: app:{{TAG}}"

# Run with Docker
docker-run TAG="latest":
  docker run --rm -it -p 3000:3000 app:{{TAG}}

# Deploy to production
deploy ENV="production":
  #!/usr/bin/env bash
  set -euo pipefail
  echo "Deploying to {{ENV}}..."
  if [ "{{ENV}}" != "production" ] && [ "{{ENV}}" != "staging" ]; then
    echo "Invalid environment: {{ENV}}"
    exit 1
  fi
  # Add your deployment commands here
  echo "✓ Deployed to {{ENV}}"

# ============================================================================
# Database
# ============================================================================

# Run database migrations
db-migrate:
  #!/usr/bin/env bash
  set -euo pipefail
  if [ -f "manage.py" ]; then
    python manage.py migrate
  elif command -v prisma >/dev/null; then
    prisma migrate deploy
  elif command -v drizzle-kit >/dev/null; then
    drizzle-kit push:mysql
  elif [ -f "migrations" ]; then
    echo "Running migrations..."
  else
    echo "No migration configuration found"
  fi

# Reset database
db-reset:
  #!/usr/bin/env bash
  set -euo pipefail
  echo "⚠️  This will reset the database. Continue? (y/N)"
  read -r response
  if [ "$response" = "y" ]; then
    just db-migrate
    echo "✓ Database reset"
  else
    echo "Cancelled"
  fi

# ============================================================================
# Git & Release
# ============================================================================

# Create a new commit with conventional commit format
commit MESSAGE:
  git add -A
  git commit -m "{{MESSAGE}}"

# Create a new release
release TYPE="patch":
  #!/usr/bin/env bash
  set -euo pipefail
  if command -v changeset >/dev/null; then
    changeset version
    changeset publish
  elif [ -f "package.json" ]; then
    npm version {{TYPE}}
    git push --follow-tags
  else
    echo "No release configuration found"
  fi

# ============================================================================
# Utilities
# ============================================================================

# Clean build artifacts and dependencies
clean:
  #!/usr/bin/env bash
  set -euo pipefail
  echo "Cleaning project..."
  rm -rf dist build target bin coverage .coverage htmlcov node_modules .venv __pycache__
  find . -type d -name "*.egg-info" -exec rm -rf {} + 2>/dev/null || true
  find . -type f -name "*.pyc" -delete 2>/dev/null || true
  echo "✓ Project cleaned"

# Setup development environment
setup:
  @just install
  @just lint || true
  @just test || true
  echo "✓ Development environment ready"

# Watch files and run tests
watch:
  #!/usr/bin/env bash
  set -euo pipefail
  if command -v watchexec >/dev/null; then
    watchexec -e py,js,ts,go,rs,jsx,tsx just test
  elif command -v entr >/dev/null; then
    find . -name "*.py" -o -name "*.js" -o -name "*.ts" | entr -c just test
  else
    echo "Install watchexec or entr for file watching"
    exit 1
  fi

# Run benchmarks
bench:
  #!/usr/bin/env bash
  set -euo pipefail
  if command -v hyperfine >/dev/null; then
    hyperfine --warmup 3 'just test'
  elif [ -f "go.mod" ]; then
    go test -bench=. ./...
  elif [ -f "Cargo.toml" ]; then
    cargo bench
  else
    echo "No benchmark configuration found"
  fi

# Open project documentation
docs:
  #!/usr/bin/env bash
  set -euo pipefail
  if [ -f "docs/index.html" ]; then
    open docs/index.html || xdg-open docs/index.html
  elif [ -f "README.md" ]; then
    cat README.md | less
  else
    echo "No documentation found"
  fi

# Show project information
info:
  @echo "Project: $(basename $(pwd))"
  @echo "Node: $(node -v 2>/dev/null || echo 'not installed')"
  @echo "Python: $(python --version 2>/dev/null || echo 'not installed')"
  @echo "Go: $(go version 2>/dev/null || echo 'not installed')"
  @echo "Rust: $(rustc --version 2>/dev/null || echo 'not installed')"
  @git rev-parse --short HEAD 2>/dev/null || echo "Not a git repository"

# Generate AI command registry
commands:
  #!/usr/bin/env bash
  set -euo pipefail
  if [ -f "scripts/generate_command_registry.sh" ]; then
    ./scripts/generate_command_registry.sh
    echo "✓ Command registry updated: docs/agent-commands.md"
  else
    echo "Command registry generator not found"
    echo "Run: curl -sL https://raw.githubusercontent.com/Softsensor-org/DevPilot/main/scripts/generate_command_registry.sh > scripts/generate_command_registry.sh"
  fi

# After-clone playbook: Complete repo review
after-clone:
  @echo "🚀 Running after-clone playbook..."
  @scripts/doctor.sh || true
  @scripts/apply_profile.sh --skill l1 --phase mvp || true
  @scripts/system_build.sh || true
  @just review-repo
  @echo "✅ Playbook complete! Check artifacts/ for reports"

# Repository review: Complete analysis (one-liner version)
repo-review:
    @mkdir -p artifacts
    @./scripts/pack_context.sh 2>/dev/null || true
    @just fmt 2>/dev/null || true; just lint 2>/dev/null || true; just test-unit -k smoke 2>/dev/null || true
    @gitleaks detect -r artifacts/gitleaks.json 2>/dev/null || true
    @semgrep ci --json --output artifacts/semgrep.json 2>/dev/null || true
    @trivy fs --scanners vuln,secret,config -f json -o artifacts/trivy.json . 2>/dev/null || true
    @echo "✓ Repo review complete. Check artifacts/"

# DEPRECATED: Use repo-review above
review-repo:
  @just repo-review

# Quick security scan
security-quick:
  @echo "🔒 Quick security scan..."
  @gitleaks detect --no-git 2>/dev/null || echo "No secrets found"
  @semgrep --config=auto --severity=ERROR 2>/dev/null || echo "No critical issues"
  @trivy fs . --severity=HIGH,CRITICAL --exit-code=0 2>/dev/null || echo "No high vulnerabilities"

# Pre-PR review: AI review of changes against base branch
review-pre-pr BASE="main":
  @echo "🔍 Running pre-PR review against {{BASE}}..."
  @mkdir -p artifacts
  @git fetch --no-tags origin {{BASE}} --depth=1
  @git diff --unified=1 --minimal --no-color origin/{{BASE}}...HEAD > artifacts/review_diff.patch
  @echo "ROLE: Senior reviewer. File-scoped bullets with fixes." > artifacts/review_prompt.txt
  @echo "DIFF:" >> artifacts/review_prompt.txt
  @cat artifacts/review_diff.patch >> artifacts/review_prompt.txt
  @if command -v claude >/dev/null; then \
    claude --system-prompt system/active.md --input-file artifacts/review_prompt.txt > artifacts/review_local.txt; \
  elif command -v codex >/dev/null; then \
    codex exec --system-file system/active.md --input-file artifacts/review_prompt.txt > artifacts/review_local.txt; \
  elif command -v gemini >/dev/null; then \
    gemini generate --system-file system/active.md --prompt-file artifacts/review_prompt.txt > artifacts/review_local.txt; \
  elif command -v grok >/dev/null; then \
    grok chat --system "$$(cat system/active.md)" --input-file artifacts/review_prompt.txt > artifacts/review_local.txt; \
  else \
    echo "No AI CLI found. Install claude, codex, gemini, or grok"; \
  fi
  @echo "📝 Review saved to artifacts/review_local.txt"
  @head -50 artifacts/review_local.txt

# Review local changes with AI (one-liner version)
review-local BASE="main":
    @mkdir -p artifacts
    @git fetch --no-tags origin {{BASE}} --depth=1 2>/dev/null || true
    @git diff --unified=1 --minimal --no-color origin/{{BASE}}...HEAD > artifacts/review_diff.patch
    @echo "ROLE: Senior reviewer. Bulleted, file-scoped suggestions." > artifacts/review_prompt.txt
    @echo "DIFF:" >> artifacts/review_prompt.txt
    @cat artifacts/review_diff.patch >> artifacts/review_prompt.txt
    @if command -v claude >/dev/null; then \
        claude --system-prompt system/active.md --input-file artifacts/review_prompt.txt > artifacts/review_local.txt; \
    elif command -v codex >/dev/null; then \
        codex exec --system-file system/active.md --input-file artifacts/review_prompt.txt > artifacts/review_local.txt; \
    elif command -v gemini >/dev/null; then \
        gemini generate --system-file system/active.md --prompt-file artifacts/review_prompt.txt > artifacts/review_local.txt; \
    elif command -v grok >/dev/null; then \
        grok chat --system "$$(cat system/active.md)" --input-file artifacts/review_prompt.txt > artifacts/review_local.txt; \
    else \
        echo "No AI CLI found. Install claude, codex, gemini, or grok"; exit 1; \
    fi
    @echo "✓ Wrote artifacts/review_local.txt"
    @head -30 artifacts/review_local.txt

# Generate tickets from codebase (one-liner version)
tickets:
    @mkdir -p artifacts
    @cat .claude/commands/tickets-from-code.md > artifacts/tickets_prompt.txt 2>/dev/null || echo "Analyze codebase and generate tickets in JSON format" > artifacts/tickets_prompt.txt
    @if command -v claude >/dev/null; then \
        claude --system-prompt system/active.md --input-file artifacts/tickets_prompt.txt > artifacts/tickets.json; \
    elif command -v codex >/dev/null; then \
        codex exec --system-file system/active.md --input-file artifacts/tickets_prompt.txt > artifacts/tickets.json; \
    elif command -v gemini >/dev/null; then \
        gemini generate --system-file system/active.md --prompt-file artifacts/tickets_prompt.txt > artifacts/tickets.json; \
    elif command -v grok >/dev/null; then \
        grok chat --system "$$(cat system/active.md)" --input-file artifacts/tickets_prompt.txt > artifacts/tickets.json; \
    else \
        echo "No AI CLI found. Install claude, codex, gemini, or grok"; exit 1; \
    fi
    @jq -r '.tickets[] | [ .id,.title,.type,.priority,.effort, (.labels//[]|join("|")), (.assignee//""), (.dependencies//[]|join("|")), (.notes//""|gsub("[\r\n]+";" ")), (.acceptance_criteria//[]|join("; ")) ] | @csv' artifacts/tickets.json > artifacts/tickets.csv 2>/dev/null || echo "Could not convert to CSV"
    @echo "✓ Wrote artifacts/tickets.{json,csv}"

# DEPRECATED: Use simpler tickets target above
tickets-old:
  #!/usr/bin/env bash
  set -euo pipefail
  echo "Analyzing codebase and generating tickets..."

  # Create output directory
  mkdir -p tickets

  # Build analysis prompt (use command file if exists, else default)
  prompt="/tmp/ticket-prompt.md"
  if [ -f .claude/commands/tickets-from-code.md ]; then
    cp .claude/commands/tickets-from-code.md "$prompt"
  else
    echo "Analyze this codebase and generate tickets in JSON format:" > "$prompt"
    echo '{"tickets": [{"id": "PROJ-001", "title": "...", "type": "bug|feature|chore", "priority": "P0-P3", "effort": "XS-XL"}]}' >> "$prompt"
    find . -type f \( -name "*.js" -o -name "*.ts" -o -name "*.py" \) | head -20 >> "$prompt"
  fi

  # Try CLIs using system/active.md as system prompt and command as input
  out="tickets/tickets.json"
  if command -v claude >/dev/null; then
    claude --system-prompt system/active.md --input-file "$prompt" > "$out"
  elif command -v codex >/dev/null; then
    codex exec --system-file system/active.md --input-file "$prompt" > "$out"
  elif command -v gemini >/dev/null; then
    gemini generate --system-file system/active.md --prompt-file "$prompt" > "$out"
  elif command -v grok >/dev/null; then
    grok chat --system "$(cat system/active.md 2>/dev/null || echo 'Generate tickets')" --input-file "$prompt" > "$out"
  else
    echo "No AI CLI found. Install claude, codex, gemini, or grok"
    exit 1
  fi

  # Convert JSON to CSV (matching generate_tickets.sh format)
  if [ -s "$out" ] && jq -e . "$out" >/dev/null 2>&1; then
    jq -r '.tickets[] | [
      .id,
      .title,
      .type,
      .priority,
      .effort,
      (.labels // [] | join("|")),
      (.assignee // ""),
      (.dependencies // [] | join("|")),
      (.notes // "" | gsub("[\r\n]+"; " ")),
      (.acceptance_criteria // [] | join("; "))
    ] | @csv' "$out" > tickets/backlog.csv
    echo "✓ Generated tickets: tickets/backlog.csv"
  else
    echo "Failed to generate valid JSON"
  fi

# Aliases for common commands
alias t := test
alias l := lint
alias f := format
alias b := build
alias r := run
alias d := dev
