#!/bin/bash
set -euo pipefail

# dp-apiize: Convert ML model to FastAPI service
ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"

# Utility functions
say() { printf "%s\n" "$*"; }

cmd_from_model() {
  local model_path="${1:-}"
  local output_dir="${2:-api_service}"
  local force="${3:-}"

  [[ -z "$model_path" ]] && {
    say "Usage: dp apiize from-model <model-path> [output-dir] [--force]"
    say "Example: dp apiize from-model model.pt api_service"
    exit 1
  }

  [[ -f "$model_path" ]] || {
    say "Error: Model file not found: $model_path"
    exit 1
  }

  # Check for existing output directory
  if [[ -d "$output_dir" ]] && [[ "$force" != "--force" ]]; then
    say "Error: Directory $output_dir already exists. Use --force to overwrite."
    exit 1
  fi

  # Detect model framework
  local framework=""
  case "${model_path##*.}" in
    pt|pth|safetensors) framework="torch" ;;
    onnx) framework="onnx" ;;
    h5|keras|pb) framework="tf" ;;
    pkl|joblib) framework="sklearn" ;;
    json|xgb) framework="xgboost" ;;
    *)
      say "Warning: Unknown model extension. Defaulting to torch."
      framework="torch"
      ;;
  esac

  say "▸ Converting $framework model to API service..."
  say "  Model: $model_path"
  say "  Output: $output_dir/"

  # Create service structure
  rm -rf "$output_dir" 2>/dev/null || true
  mkdir -p "$output_dir"/{app,models,tests,docker}

  # Copy model
  cp "$model_path" "$output_dir/models/"
  local model_filename="${model_path##*/}"

  # Generate improved FastAPI app with all fixes
  cat > "$output_dir/app/main.py" <<'FASTAPI'
from fastapi import FastAPI, HTTPException, Header, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import os
import time
import uuid
import logging
from typing import Dict, Any, List, Optional
from contextlib import nullcontext

logging.basicConfig(level=os.getenv("LOG_LEVEL", "INFO"))
logger = logging.getLogger(__name__)

app = FastAPI(title="Model API Service", version="1.0.0")

# CORS configuration (optional, via env)
cors_origins = os.getenv("CORS_ORIGINS", "").split(",") if os.getenv("CORS_ORIGINS") else []
if cors_origins:
    app.add_middleware(
        CORSMiddleware,
        allow_origins=cors_origins,
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

# Optional API key (set via env)
API_KEY = os.getenv("API_KEY")

# Model globals
model = None
device = None

def load_model():
    global model, device
    model_path = os.getenv("MODEL_PATH", "models/MODEL_FILE")
    if not os.path.exists(model_path):
        logger.warning(f"Model file not found: {model_path}")
        return

    # Framework-specific loading will be added below
    logger.info("Model loading...")

@app.on_event("startup")
async def startup():
    load_model()

class PredictRequest(BaseModel):
    data: List[List[float]]

class PredictResponse(BaseModel):
    predictions: List[Any]
    model_version: str = "1.0.0"
    request_id: Optional[str] = None
    latency_ms: Optional[float] = None

@app.get("/")
async def root():
    return {"message": "Model API Service", "docs": "/docs", "health": "/health"}

@app.get("/health")
@app.get("/healthz")
async def health():
    return {"status": "healthy", "model_loaded": model is not None}

@app.post("/predict", response_model=PredictResponse)
@app.post("/infer", response_model=PredictResponse)
async def predict(
    request: PredictRequest,
    x_request_id: Optional[str] = Header(None),
    x_api_key: Optional[str] = Header(None)
):
    # Check API key if configured
    if API_KEY and x_api_key != API_KEY:
        raise HTTPException(status_code=401, detail="Invalid API key")

    # Generate or use request ID
    request_id = x_request_id or str(uuid.uuid4())[:16]

    if model is None:
        raise HTTPException(status_code=503, detail="Model not loaded")

    start_time = time.time()

    try:
        # Framework-specific prediction (will be added below)
        predictions = predict_with_model(request.data)

        latency_ms = (time.time() - start_time) * 1000

        return PredictResponse(
            predictions=predictions,
            request_id=request_id,
            latency_ms=latency_ms
        )
    except Exception as e:
        logger.error(f"Prediction error for request {request_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

def predict_with_model(data):
    """Framework-specific prediction logic"""
    # Will be replaced by framework-specific implementation
    return [[0.0] for _ in data]
FASTAPI

  # Add framework-specific implementations
  case "$framework" in
    torch)
      cat >> "$output_dir/app/main.py" <<TORCH

import torch
import numpy as np

def load_model():
    global model, device
    model_path = os.getenv("MODEL_PATH", "models/$model_filename")
    if not os.path.exists(model_path):
        logger.warning(f"Model file not found: {model_path}")
        return

    # Detect device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"Using device: {device}")

    # Load model
    try:
        if model_path.endswith((".pt", ".pth")):
            model = torch.jit.load(model_path, map_location=device)
        else:
            model = torch.load(model_path, map_location=device)

        # Move model to device
        model = model.to(device)
        model.eval()
        logger.info(f"PyTorch model loaded from {model_path}")
    except Exception as e:
        logger.error(f"Failed to load model: {e}")

def predict_with_model(data):
    """PyTorch prediction with proper device handling"""
    import torch

    # Convert to tensor and move to device
    x = torch.tensor(data, dtype=torch.float32).to(device)

    # Use autocast for GPU if available
    amp_enabled = torch.cuda.is_available() and str(device).startswith("cuda")
    ctx = torch.autocast(device_type="cuda") if amp_enabled else nullcontext()

    with torch.no_grad():
        with ctx:
            y = model(x)

    # Convert back to list
    if hasattr(y, "detach"):
        return y.detach().cpu().numpy().tolist()
    return y
TORCH
      ;;

    onnx)
      cat >> "$output_dir/app/main.py" <<ONNX

import onnxruntime as ort
import numpy as np

def load_model():
    global model
    model_path = os.getenv("MODEL_PATH", "models/$model_filename")
    if not os.path.exists(model_path):
        logger.warning(f"Model file not found: {model_path}")
        return

    try:
        # Check for GPU availability
        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if ort.get_device() == 'GPU' else ['CPUExecutionProvider']
        model = ort.InferenceSession(model_path, providers=providers)
        logger.info(f"ONNX model loaded from {model_path} with providers: {providers}")
    except Exception as e:
        logger.error(f"Failed to load model: {e}")

def predict_with_model(data):
    """ONNX prediction with shape normalization"""
    import numpy as np

    # Convert to numpy and ensure batch dimension
    x = np.array(data, dtype=np.float32)
    if x.ndim == 1:
        x = x[np.newaxis, ...]

    # Get input name
    input_name = model.get_inputs()[0].name

    # Run inference
    outputs = model.run(None, {input_name: x})

    # Normalize output to list format
    if isinstance(outputs, (list, tuple)):
        return [np.asarray(o).tolist() for o in outputs]
    return np.asarray(outputs).tolist()
ONNX
      ;;

    tf)
      cat >> "$output_dir/app/main.py" <<TENSORFLOW

import tensorflow as tf
import numpy as np

def load_model():
    global model
    model_path = os.getenv("MODEL_PATH", "models/$model_filename")
    if not os.path.exists(model_path):
        logger.warning(f"Model file not found: {model_path}")
        return

    try:
        # Load model (handles both SavedModel and H5 formats)
        if model_path.endswith('.pb'):
            model = tf.saved_model.load(os.path.dirname(model_path))
        else:
            model = tf.keras.models.load_model(model_path)
        logger.info(f"TensorFlow model loaded from {model_path}")
    except Exception as e:
        logger.error(f"Failed to load model: {e}")

def predict_with_model(data):
    """TensorFlow prediction with output normalization"""
    import tensorflow as tf

    # Convert to tensor
    x = tf.convert_to_tensor(data, dtype=tf.float32)

    # Handle different model types
    if hasattr(model, 'signatures'):
        # SavedModel format
        fn = model.signatures.get("serving_default") or list(model.signatures.values())[0]
        outputs = fn(x)

        # Normalize output
        try:
            if isinstance(outputs, dict):
                return {k: v.numpy().tolist() for k, v in outputs.items()}
            else:
                return outputs.numpy().tolist()
        except Exception:
            return str(outputs)
    else:
        # Keras model
        outputs = model(x)
        return outputs.numpy().tolist()
TENSORFLOW
      ;;

    sklearn)
      cat >> "$output_dir/app/main.py" <<SKLEARN

import joblib
import pickle
import numpy as np

def load_model():
    global model
    model_path = os.getenv("MODEL_PATH", "models/$model_filename")
    if not os.path.exists(model_path):
        logger.warning(f"Model file not found: {model_path}")
        return

    try:
        # Try joblib first, then pickle
        if model_path.endswith('.joblib'):
            model = joblib.load(model_path)
        else:
            with open(model_path, 'rb') as f:
                model = pickle.load(f)
        logger.info(f"Scikit-learn model loaded from {model_path}")
    except Exception as e:
        logger.error(f"Failed to load model: {e}")

def predict_with_model(data):
    """Scikit-learn prediction"""
    import numpy as np

    # Convert to numpy array
    x = np.array(data)

    # Make prediction
    predictions = model.predict(x)

    # Convert to list
    return predictions.tolist()
SKLEARN
      ;;

    xgboost)
      cat >> "$output_dir/app/main.py" <<XGBOOST

import xgboost as xgb
import numpy as np

def load_model():
    global model
    model_path = os.getenv("MODEL_PATH", "models/$model_filename")
    if not os.path.exists(model_path):
        logger.warning(f"Model file not found: {model_path}")
        return

    try:
        model = xgb.Booster()
        model.load_model(model_path)
        logger.info(f"XGBoost model loaded from {model_path}")
    except Exception as e:
        logger.error(f"Failed to load model: {e}")

def predict_with_model(data):
    """XGBoost prediction"""
    import numpy as np
    import xgboost as xgb

    # Convert to DMatrix
    x = np.array(data)
    dmatrix = xgb.DMatrix(x)

    # Make prediction
    predictions = model.predict(dmatrix)

    # Convert to list
    return predictions.tolist()
XGBOOST
      ;;
  esac

  # Generate improved requirements.txt without conflicts
  cat > "$output_dir/requirements.txt" <<'REQS'
fastapi>=0.111.0
uvicorn[standard]>=0.30.0
pydantic>=2.7.0
python-multipart>=0.0.9
prometheus-fastapi-instrumentator>=7.0.0
REQS

  # Add framework-specific requirements with proper versioning
  case "$framework" in
    torch)
      cat >> "$output_dir/requirements.txt" <<'TORCH_REQS'
# For GPU: Install PyTorch with CUDA support from https://pytorch.org/get-started/locally/
# Example: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
torch>=2.1.0
numpy>=1.24.0
TORCH_REQS
      ;;
    onnx)
      cat >> "$output_dir/requirements.txt" <<'ONNX_REQS'
onnxruntime>=1.18.0
# For GPU: pip install onnxruntime-gpu instead
numpy>=1.24.0
ONNX_REQS
      ;;
    tf)
      cat >> "$output_dir/requirements.txt" <<'TF_REQS'
# TensorFlow includes its own numpy version
tensorflow>=2.15.0
# For GPU: tensorflow[and-cuda]>=2.15.0
TF_REQS
      ;;
    sklearn)
      cat >> "$output_dir/requirements.txt" <<'SKLEARN_REQS'
scikit-learn>=1.4.0
joblib>=1.3.2
numpy>=1.24.0
SKLEARN_REQS
      ;;
    xgboost)
      cat >> "$output_dir/requirements.txt" <<'XGB_REQS'
xgboost>=2.0.0
numpy>=1.24.0
XGB_REQS
      ;;
  esac

  # Generate Dockerfile
  cat > "$output_dir/docker/Dockerfile" <<'DOCKER'
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies if needed
RUN apt-get update && apt-get install -y --no-install-recommends \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY app/ ./app/
COPY models/ ./models/

# Environment variables
ENV MODEL_PATH=/app/models/model
ENV LOG_LEVEL=INFO

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Run server
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
DOCKER

  # GPU support Dockerfile for torch/tf
  if [[ "$framework" == "torch" ]] || [[ "$framework" == "tf" ]]; then
    cat > "$output_dir/docker/Dockerfile.gpu" <<'GPUDOCKER'
FROM nvidia/cuda:12.1.0-runtime-ubuntu22.04

# Install Python
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.11 \
    python3-pip \
    curl \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Install Python dependencies
COPY requirements.txt .
RUN pip3 install --no-cache-dir -r requirements.txt

# Copy application
COPY app/ ./app/
COPY models/ ./models/

# Environment variables
ENV MODEL_PATH=/app/models/model
ENV LOG_LEVEL=INFO
ENV CUDA_VISIBLE_DEVICES=0

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Run server
CMD ["python3", "-m", "uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
GPUDOCKER
  fi

  # Generate improved test file
  cat > "$output_dir/tests/test_api.py" <<'TEST'
import os
import pytest
from fastapi.testclient import TestClient

# Import app after setting test env
os.environ["MODEL_PATH"] = os.environ.get("MODEL_PATH", "")
from app.main import app

client = TestClient(app)

def test_health():
    """Test health endpoint always works"""
    response = client.get("/health")
    assert response.status_code == 200
    data = response.json()
    assert "status" in data
    assert data["status"] == "healthy"

def test_healthz():
    """Test k8s-style health endpoint"""
    response = client.get("/healthz")
    assert response.status_code == 200

def test_root():
    """Test root endpoint"""
    response = client.get("/")
    assert response.status_code == 200
    data = response.json()
    assert "message" in data
    assert "docs" in data

def test_docs():
    """Test OpenAPI docs are available"""
    response = client.get("/docs")
    assert response.status_code == 200

def test_predict_no_model():
    """Test predict fails gracefully without model"""
    if not os.getenv("MODEL_PATH") or not os.path.exists(os.getenv("MODEL_PATH", "")):
        # Skip actual inference test if no model
        response = client.get("/health")
        assert response.status_code == 200
        return

    # If model exists, test prediction
    response = client.post("/predict", json={"data": [[1.0, 2.0, 3.0]]})
    assert response.status_code in [200, 503]  # OK or Service Unavailable

def test_predict_with_request_id():
    """Test request ID tracking"""
    headers = {"X-Request-Id": "test-123"}

    # This will fail without model, but we're testing the headers work
    response = client.post(
        "/predict",
        json={"data": [[1.0, 2.0, 3.0]]},
        headers=headers
    )

    # Should get 503 if no model, but headers should process
    if response.status_code == 200:
        data = response.json()
        assert "request_id" in data
        assert data["request_id"] == "test-123"
TEST

  # Generate docker-compose.yml (CPU by default)
  cat > "$output_dir/docker-compose.yml" <<COMPOSE
version: '3.8'

services:
  api:
    build:
      context: .
      dockerfile: docker/Dockerfile
      # For GPU: dockerfile: docker/Dockerfile.gpu
    ports:
      - "8000:8000"
    environment:
      - MODEL_PATH=/app/models/$model_filename
      - LOG_LEVEL=INFO
      - CORS_ORIGINS=http://localhost:3000,http://localhost:8080
      # - API_KEY=your-secret-key  # Uncomment to enable API key auth
    volumes:
      - ./models:/app/models:ro
    # For GPU: Uncomment below
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

# Optional: Add monitoring
  # prometheus:
  #   image: prom/prometheus:latest
  #   volumes:
  #     - ./prometheus.yml:/etc/prometheus/prometheus.yml
  #   ports:
  #     - "9090:9090"
COMPOSE

  # Generate improved README
  cat > "$output_dir/README.md" <<README
# Model API Service

FastAPI service wrapping ML model for inference.

## Quick Start

\`\`\`bash
# Install dependencies
pip install -r requirements.txt

# Set model path (optional, defaults to models/$model_filename)
export MODEL_PATH=models/$model_filename

# Run locally
uvicorn app.main:app --reload

# Or use Docker
docker-compose up
\`\`\`

## GPU Support

For GPU inference:

\`\`\`bash
# Build GPU image
docker build -f docker/Dockerfile.gpu -t model-api-gpu .

# Run with GPU
docker run --gpus all -p 8000:8000 \\
  -e MODEL_PATH=/app/models/$model_filename \\
  model-api-gpu

# Or with compose (uncomment GPU sections in docker-compose.yml)
docker-compose up
\`\`\`

## API Endpoints

- \`GET /\` - Service info
- \`GET /health\` - Health check
- \`GET /healthz\` - K8s-style health check
- \`POST /predict\` - Model inference
- \`POST /infer\` - Alias for /predict
- \`GET /docs\` - Interactive OpenAPI docs
- \`GET /openapi.json\` - OpenAPI schema

## Request/Response Format

### Request
\`\`\`json
{
  "data": [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]
}
\`\`\`

### Response
\`\`\`json
{
  "predictions": [[0.5], [0.8]],
  "model_version": "1.0.0",
  "request_id": "abc123",
  "latency_ms": 12.5
}
\`\`\`

## Configuration

Environment variables:
- \`MODEL_PATH\` - Path to model file (default: models/$model_filename)
- \`LOG_LEVEL\` - Logging level (default: INFO)
- \`CORS_ORIGINS\` - Comma-separated CORS origins
- \`API_KEY\` - Optional API key for authentication

## Testing

\`\`\`bash
# Run tests
pytest tests/ -v

# With coverage
pytest tests/ --cov=app --cov-report=html
\`\`\`

## Production Deployment

1. Build optimized image:
   \`\`\`bash
   docker build -f docker/Dockerfile -t model-api:latest .
   \`\`\`

2. Run with proper limits:
   \`\`\`bash
   docker run -d \\
     --name model-api \\
     --restart unless-stopped \\
     -p 8000:8000 \\
     -m 2g --cpus 2 \\
     -e MODEL_PATH=/app/models/$model_filename \\
     -e API_KEY=\$API_KEY \\
     model-api:latest
   \`\`\`

3. For Kubernetes, use the generated manifests or Helm charts.

## Monitoring

The service includes Prometheus metrics at \`/metrics\`.

## Security

- Set \`API_KEY\` environment variable to enable authentication
- Configure \`CORS_ORIGINS\` for your domains
- Use HTTPS in production (reverse proxy recommended)
- Implement rate limiting at ingress level
README

  # Generate Makefile
  cat > "$output_dir/Makefile" <<'MAKEFILE'
.PHONY: install dev test docker docker-gpu run clean format lint

install:
	pip install -r requirements.txt

dev:
	uvicorn app.main:app --reload --port 8000

test:
	pytest tests/ -v

test-cov:
	pytest tests/ --cov=app --cov-report=html --cov-report=term

docker:
	docker build -f docker/Dockerfile -t model-api .

docker-gpu:
	docker build -f docker/Dockerfile.gpu -t model-api-gpu .

run:
	docker run --rm -p 8000:8000 model-api

run-gpu:
	docker run --rm --gpus all -p 8000:8000 model-api-gpu

format:
	black app/ tests/
	isort app/ tests/

lint:
	pylint app/
	mypy app/

clean:
	find . -type d -name __pycache__ -exec rm -rf {} +
	find . -type f -name "*.pyc" -delete
	rm -rf .pytest_cache .coverage htmlcov

openapi:
	python -c "from app.main import app; import json; print(json.dumps(app.openapi(), indent=2))" > openapi.json
MAKEFILE

  # Try to generate OpenAPI spec (optional, may fail if deps not installed)
  cat > "$output_dir/export_openapi.py" <<'EXPORT'
#!/usr/bin/env python3
try:
    from app.main import app
    import json
    import yaml

    openapi_schema = app.openapi()

    # Write JSON version
    with open("openapi.json", "w") as f:
        json.dump(openapi_schema, f, indent=2)

    # Write YAML version if pyyaml available
    try:
        with open("openapi.yaml", "w") as f:
            yaml.safe_dump(openapi_schema, f, sort_keys=False)
        print("✓ Exported openapi.json and openapi.yaml")
    except ImportError:
        print("✓ Exported openapi.json (install pyyaml for YAML output)")

except Exception as e:
    print(f"Could not export OpenAPI spec: {e}")
    print("Run 'make openapi' after installing dependencies")
EXPORT

  chmod +x "$output_dir/export_openapi.py"

  say "✓ API service scaffolded in $output_dir/"
  say ""
  say "Next steps:"
  say "  1. cd $output_dir"
  say "  2. pip install -r requirements.txt"

  if [[ "$framework" == "torch" ]]; then
    say "  3. For GPU: pip install torch --index-url https://download.pytorch.org/whl/cu121"
  fi

  say "  4. uvicorn app.main:app --reload"
  say "  5. Open http://localhost:8000/docs"
  say ""
  say "For production:"
  say "  docker-compose up        # CPU mode"

  if [[ "$framework" == "torch" ]] || [[ "$framework" == "tf" ]]; then
    say "  docker build -f docker/Dockerfile.gpu -t api-gpu . && docker run --gpus all -p 8000:8000 api-gpu  # GPU mode"
  fi

  say ""
  say "To export OpenAPI spec: cd $output_dir && ./export_openapi.py"
}

cmd_scaffold() {
  # General API scaffolding (future extension)
  say "API scaffolding coming soon. Use 'dp apiize from-model' for ML models."
}

# Auto-detect models in current directory
cmd_detect() {
  local output_dir="${1:-api_service}"

  say "▸ Searching for ML models in current directory..."

  local model_path=""
  local framework=""

  # Use null-safe find to detect models (handles spaces in filenames)
  while IFS= read -r -d '' f; do
    framework="onnx"
    model_path="$f"
    break
  done < <(find . -maxdepth 3 -type f -name '*.onnx' -print0 2>/dev/null)

  if [[ -z "$model_path" ]]; then
    while IFS= read -r -d '' f; do
      case "$f" in
        *.safetensors|*.pt|*.pth)
          framework="torch"
          model_path="$f"
          break
          ;;
      esac
    done < <(find . -maxdepth 3 -type f \( -name '*.safetensors' -o -name '*.pt' -o -name '*.pth' \) -print0 2>/dev/null)
  fi

  if [[ -z "$model_path" ]]; then
    while IFS= read -r -d '' f; do
      if [[ -f "$f" ]]; then
        framework="tf"
        model_path="$(dirname "$f")"
        break
      fi
    done < <(find . -maxdepth 3 -type f -name 'saved_model.pb' -print0 2>/dev/null)
  fi

  if [[ -z "$model_path" ]]; then
    while IFS= read -r -d '' f; do
      framework="sklearn"
      model_path="$f"
      break
    done < <(find . -maxdepth 3 -type f \( -name '*.joblib' -o -name '*.pkl' \) -print0 2>/dev/null)
  fi

  if [[ -z "$model_path" ]]; then
    while IFS= read -r -d '' f; do
      if grep -q '"learner"' "$f" 2>/dev/null; then
        framework="xgboost"
        model_path="$f"
        break
      fi
    done < <(find . -maxdepth 3 -type f -name '*.json' -print0 2>/dev/null)
  fi

  if [[ -n "$model_path" ]]; then
    say "✓ Found $framework model: $model_path"
    say ""
    cmd_from_model "$model_path" "$output_dir"
  else
    say "No ML models found in current directory."
    say "Searched for: *.onnx, *.pt, *.pth, *.safetensors, saved_model.pb, *.pkl, *.joblib, *.json"
    say ""
    say "To manually specify a model:"
    say "  dp apiize from-model <model-path> [output-dir]"
  fi
}

# Main command router
case "${1:-}" in
  from-model) shift; cmd_from_model "$@" ;;
  detect) shift; cmd_detect "$@" ;;
  scaffold) shift; cmd_scaffold "$@" ;;
  *)
    cat <<'HELP'
dp apiize - Convert models to API services

Usage:
  dp apiize from-model <model-path> [output-dir] [--force]  # Convert specific model
  dp apiize detect [output-dir]                              # Auto-detect and convert
  dp apiize scaffold <type> [options]                        # General API scaffold (coming soon)

Examples:
  dp apiize from-model model.pt api_service        # PyTorch model
  dp apiize from-model model.onnx api_service      # ONNX model
  dp apiize from-model model.pkl api_service       # Scikit-learn model
  dp apiize detect                                  # Find and convert model in current dir

Supported frameworks:
  - PyTorch (.pt, .pth, .safetensors)
  - ONNX (.onnx)
  - TensorFlow/Keras (.h5, .keras, .pb)
  - Scikit-learn (.pkl, .joblib)
  - XGBoost (.json, .xgb)

Output includes:
  - FastAPI application with /predict and /infer endpoints
  - Docker configuration (CPU and GPU variants)
  - Comprehensive test suite
  - OpenAPI documentation (exported as openapi.json/yaml)
  - Production-ready deployment configs
  - Request tracing and optional API key auth

Features:
  - Automatic framework detection
  - GPU acceleration support
  - Request ID tracking
  - CORS configuration
  - Prometheus metrics
  - Health checks
  - Batch prediction support
HELP
    ;;
esac
