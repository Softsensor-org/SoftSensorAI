#!/usr/bin/env bash
set -euo pipefail

# Test generation orchestrator for DevPilot
# Detect multi-user vs single-user installation
if [[ -f "/opt/devpilot/etc/devpilot.conf" ]]; then
  source /opt/devpilot/etc/devpilot.conf
  ROOT="${DEVPILOT_ROOT:-/opt/devpilot}"
  ART="${DEVPILOT_ARTIFACTS:-${DEVPILOT_USER_DIR:-$HOME/.devpilot}/artifacts}"
else
  ROOT="$(git rev-parse --show-toplevel 2>/dev/null || cd "$(dirname "$0")/.." && pwd)"
  ART="$ROOT/artifacts"
fi
SHIM="$ROOT/tools/ai_shim.sh"

# Support functions
say() { echo "[testgen] $1" >&2; }
have() { command -v "$1" >/dev/null 2>&1; }
now() { date -Iseconds 2>/dev/null || date '+%Y-%m-%dT%H:%M:%S'; }

# Extract JSON from AI output
extract_json() {
  awk 'BEGIN{in=0} /^[[:space:]]*{/ && !in {in=1} in{print}' | \
  awk 'BEGIN{c=0} {print; for(i=1;i<=length($0);i++){ch=substr($0,i,1); if(ch=="{")c++; else if(ch=="}")c--;}} END{exit c!=0}'
}

# Detect language and test runner (supports monorepos)
detect_stack() {
  local lang="unknown" runner="unknown"
  local has_py=0 has_js=0 has_go=0 has_java=0

  # Check for multiple languages (monorepo)
  [ -f "requirements.txt" ] || [ -f "pyproject.toml" ] || [ -f "setup.py" ] && has_py=1
  [ -f "package.json" ] && has_js=1
  [ -f "go.mod" ] && has_go=1
  [ -f "pom.xml" ] || [ -f "build.gradle" ] || [ -f "build.gradle.kts" ] && has_java=1

  # Determine language(s) and runner(s)
  local lang_count=$((has_py + has_js + has_go + has_java))

  if [ "$lang_count" -gt 1 ]; then
    lang="mixed"
    runner="multi"
  elif [ "$has_js" -eq 1 ]; then
    lang="javascript"
    if grep -q '"jest"' package.json 2>/dev/null; then
      runner="jest"
    elif grep -q '"vitest"' package.json 2>/dev/null; then
      runner="vitest"
    else
      runner="npm-test"
    fi
  elif [ "$has_py" -eq 1 ]; then
    lang="python"
    runner="pytest"
  elif [ "$has_go" -eq 1 ]; then
    lang="go"
    runner="go-test"
  elif [ "$has_java" -eq 1 ]; then
    lang="java"
    if [ -f "pom.xml" ]; then
      runner="mvn-surefire"
    else
      runner="gradle"
    fi
  fi

  echo "{\"language\":\"$lang\",\"runner\":\"$runner\"}"
}

# Find test directories
find_test_dirs() {
  local dirs=""
  [ -d "tests" ] && dirs="tests"
  [ -d "test" ] && dirs="${dirs:+$dirs,}test"
  [ -d "__tests__" ] && dirs="${dirs:+$dirs,}__tests__"
  [ -d "src/test" ] && dirs="${dirs:+$dirs,}src/test"
  echo "${dirs:-tests}"
}

# Main test generation function
generate_tests() {
  local diff_file="${1:-}"
  local coverage_file="${2:-}"
  local risk_tags="${3:-}"
  local contract_file="${4:-}"

  # Create task directory
  local task_id="testgen-$(date +%Y%m%d%H%M%S)-$(head -c 4 /dev/urandom | xxd -p)"
  local dir="$ART/testgen/$task_id"
  mkdir -p "$dir"

  say "▸ Task ID: $task_id"
  say "▸ Working directory: $dir"

  # Detect environment
  local stack_info
  stack_info=$(detect_stack)
  local language runner test_dirs
  language=$(echo "$stack_info" | jq -r '.language')
  runner=$(echo "$stack_info" | jq -r '.runner')
  test_dirs=$(find_test_dirs)

  say "▸ Detected: $language with $runner, test dirs: $test_dirs"

  # Prepare context for orchestrator
  local context_file="$dir/context.json"
  cat > "$context_file" <<JSON
{
  "language": "$language",
  "runner": "$runner",
  "test_dirs": "$test_dirs",
  "risk_tags": "$risk_tags",
  "has_diff": $([ -n "$diff_file" ] && echo "true" || echo "false"),
  "has_coverage": $([ -n "$coverage_file" ] && echo "true" || echo "false"),
  "has_contract": $([ -n "$contract_file" ] && echo "true" || echo "false")
}
JSON

  # Build orchestrator prompt
  local orchestrator_prompt="$dir/orchestrator_prompt.txt"
  {
    echo "CONTEXT:"
    cat "$context_file"
    echo

    if [ -n "$diff_file" ] && [ -f "$diff_file" ]; then
      echo "DIFF (first 1000 lines):"
      head -n 1000 "$diff_file"
      echo
    fi

    if [ -n "$coverage_file" ] && [ -f "$coverage_file" ]; then
      echo "COVERAGE:"
      cat "$coverage_file"
      echo
    fi

    if [ -n "$contract_file" ] && [ -f "$contract_file" ]; then
      echo "CONTRACT: $contract_file"
      echo
    fi

    echo "RISK_TAGS: ${risk_tags:-none}"
    echo
    echo "INSTRUCTIONS:"
    cat "$ROOT/.claude/commands/testgen/testgen-orchestrator.md"
  } > "$orchestrator_prompt"

  # Call orchestrator
  say "▸ Planning test generation strategy..."
  local orchestrator_out="$dir/orchestrator_out.txt"
  local orchestrator_json="$dir/orchestrator.json"

  if [ -f "$SHIM" ]; then
    "$SHIM" --provider claude --model "${AI_MODEL_CLAUDE:-claude-3-7-sonnet-20250219}" \
      --prompt-file "$orchestrator_prompt" > "$orchestrator_out" 2>/dev/null || \
    "$SHIM" --provider codex --model "${AI_MODEL_CODEX:-codex-latest}" \
      --prompt-file "$orchestrator_prompt" > "$orchestrator_out" 2>/dev/null || \
    "$SHIM" --provider gemini --model "${AI_MODEL_GEMINI:-gemini-1.5-pro-latest}" \
      --prompt-file "$orchestrator_prompt" > "$orchestrator_out" 2>/dev/null || \
    "$SHIM" --provider grok --model "${AI_MODEL_GROK:-grok-2-latest}" \
      --prompt-file "$orchestrator_prompt" > "$orchestrator_out" 2>/dev/null || true
  fi

  if extract_json < "$orchestrator_out" > "$orchestrator_json"; then
    say "▸ Strategy determined, delegating to sub-prompts..."
  else
    say "⚠️  Could not parse orchestrator output, using fallback"
    echo '{"delegates":[{"prompt":"tests-from-diff.md","reason":"fallback"}]}' > "$orchestrator_json"
  fi

  # Execute each delegate prompt
  local i=0
  local total_files=0
  local all_commands="$dir/all_commands.sh"
  : > "$all_commands"

  while IFS= read -r delegate; do
    local prompt_name reason
    prompt_name=$(echo "$delegate" | jq -r '.prompt')
    reason=$(echo "$delegate" | jq -r '.reason')

    say "▸ Running: $prompt_name ($reason)"

    local sub_prompt="$dir/sub_prompt_$i.txt"
    local sub_out="$dir/sub_out_$i.txt"
    local sub_json="$dir/sub_json_$i.json"

    # Build sub-prompt
    {
      echo "CONTEXT:"
      cat "$context_file"
      echo

      if [ -n "$diff_file" ] && [ -f "$diff_file" ]; then
        echo "DIFF:"
        cat "$diff_file"
        echo
      fi

      echo "LANGUAGE: $language"
      echo "RUNNER: $runner"
      echo "TEST_DIRS: $test_dirs"
      echo "RISK_TAGS: ${risk_tags:-none}"
      echo
      echo "INSTRUCTIONS:"
      cat "$ROOT/.claude/commands/testgen/$prompt_name"
    } > "$sub_prompt"

    # Call sub-prompt with full fallback chain
    if [ -f "$SHIM" ]; then
      "$SHIM" --provider claude --model "${AI_MODEL_CLAUDE:-claude-3-7-sonnet-20250219}" \
        --prompt-file "$sub_prompt" > "$sub_out" 2>/dev/null || \
      "$SHIM" --provider codex --model "${AI_MODEL_CODEX:-codex-latest}" \
        --prompt-file "$sub_prompt" > "$sub_out" 2>/dev/null || \
      "$SHIM" --provider gemini --model "${AI_MODEL_GEMINI:-gemini-1.5-pro-latest}" \
        --prompt-file "$sub_prompt" > "$sub_out" 2>/dev/null || \
      "$SHIM" --provider grok --model "${AI_MODEL_GROK:-grok-2-latest}" \
        --prompt-file "$sub_prompt" > "$sub_out" 2>/dev/null || true
    fi

    # Parse and apply results
    if extract_json < "$sub_out" > "$sub_json"; then
      # Validate JSON schema if ajv is available
      local schema_file="$ROOT/config/schemas/testgen_output.schema.json"
      if command -v ajv >/dev/null 2>&1 && [ -f "$schema_file" ]; then
        if ! ajv validate -s "$schema_file" -d "$sub_json" >/dev/null 2>&1; then
          say "⚠️  Invalid JSON from $prompt_name; skipping"
          continue
        fi
      elif command -v jq >/dev/null 2>&1; then
        # Basic validation with jq if ajv not available
        if ! jq -e '.files and .commands' "$sub_json" >/dev/null 2>&1; then
          say "⚠️  Malformed JSON from $prompt_name; skipping"
          continue
        fi
      fi

      # Write test files
      jq -c '.files[]?' "$sub_json" 2>/dev/null | while IFS= read -r file_spec; do
        local path content overwrite
        path=$(echo "$file_spec" | jq -r '.path')
        content=$(echo "$file_spec" | jq -r '.content')
        overwrite=$(echo "$file_spec" | jq -r '.overwrite // "create_if_absent"')

        # Validate path is in allowed test directory
        if ! echo "$path" | grep -qE '^(tests?/|__tests__/|src/test/)'; then
          say "⚠️  Skipping non-test path: $path"
          continue
        fi

        # Handle overwrite policy with safety checks
        if [ -f "$path" ]; then
          case "$overwrite" in
            create_if_absent)
              say "  → Skipping existing: $path"
              ;;
            overwrite)
              # Check for safe-overwrite marker
              if head -n 3 "$path" | grep -q 'DEVPILOT:SAFE-OVERWRITE'; then
                echo "$content" > "$path"
                say "  → Overwrote (marked safe): $path"
                total_files=$((total_files + 1))
              else
                say "  → Denied overwrite (no marker): $path"
              fi
              ;;
            append)
              echo "" >> "$path"
              echo "$content" >> "$path"
              say "  → Appended to: $path"
              total_files=$((total_files + 1))
              ;;
          esac
        else
          # New file
          mkdir -p "$(dirname "$path")"
          echo "$content" > "$path"
          say "  → Created: $path"
          total_files=$((total_files + 1))
        fi
      done

      # Collect commands
      jq -r '.commands[]?.run?' "$sub_json" 2>/dev/null >> "$all_commands"
    fi

    i=$((i + 1))
  done < <(jq -c '.delegates[]?' "$orchestrator_json")

  say "▸ Generated $total_files test files"

  # Run test commands with time budget and deterministic settings
  if [ -s "$all_commands" ]; then
    say "▸ Running test commands..."
    local test_log="$dir/test_run.log"

    # Set environment for deterministic and bounded execution
    export PYTHONHASHSEED=0
    export NODE_OPTIONS="--max-old-space-size=2048"
    export NO_NET=1
    export CI=1

    while IFS= read -r cmd; do
      # Add timeout wrapper if available
      local cmd_wrapped="$cmd"
      if command -v timeout >/dev/null 2>&1; then
        # 120s timeout for test suites
        cmd_wrapped="timeout 120s $cmd"
      fi

      # Add common test flags for speed/determinism
      case "$cmd" in
        pytest*)
          # Add pytest flags for speed and determinism
          cmd_wrapped="${cmd_wrapped} -q --tb=short --maxfail=5 -o timeout=90"
          ;;
        jest*|vitest*)
          # Add JS test flags
          cmd_wrapped="${cmd_wrapped} --maxWorkers=2 --no-coverage"
          ;;
        "go test"*)
          # Add Go test flags
          cmd_wrapped="${cmd_wrapped} -timeout=90s -short"
          ;;
      esac

      say "  → $cmd_wrapped"
      if eval "$cmd_wrapped" >> "$test_log" 2>&1; then
        say "    ✓ Passed"
      else
        say "    ✗ Failed (see $test_log)"
      fi
    done < "$all_commands"
  fi

  # Generate summary
  local summary="$dir/summary.json"
  cat > "$summary" <<JSON
{
  "task_id": "$task_id",
  "language": "$language",
  "runner": "$runner",
  "delegates_used": $(jq '.delegates | length' "$orchestrator_json"),
  "files_created": $total_files,
  "timestamp": "$(now)"
}
JSON

  say "✅ Test generation complete: $summary"
  echo "$task_id"
}

# Command-line interface
usage() {
  cat <<HELP
Usage: dp testgen [OPTIONS]

Generate tests based on code changes, contracts, and risk analysis.

Options:
  --diff FILE        Path to unified diff file
  --coverage FILE    Path to coverage report (JSON)
  --risk TAGS        Comma-separated risk tags (auth,db,ml,infra)
  --contract FILE    Path to OpenAPI/contract file
  --help            Show this help message

Examples:
  # Generate tests from recent changes
  git diff > /tmp/changes.diff
  dp testgen --diff /tmp/changes.diff

  # Generate with risk awareness
  dp testgen --diff changes.diff --risk auth,db

  # Generate contract tests
  dp testgen --contract openapi.yaml

Environment:
  AI_MODEL_CLAUDE    Claude model to use (default: claude-3-7-sonnet-20250219)
  AI_MODEL_CODEX     Codex model to use (fallback)
  AI_MODEL_GEMINI    Gemini model to use (fallback)
  AI_MODEL_GROK      Grok model to use (fallback)
HELP
}

# Parse arguments
DIFF_FILE=""
COVERAGE_FILE=""
RISK_TAGS=""
CONTRACT_FILE=""

while [[ $# -gt 0 ]]; do
  case "$1" in
    --diff)
      DIFF_FILE="$2"
      shift 2
      ;;
    --coverage)
      COVERAGE_FILE="$2"
      shift 2
      ;;
    --risk)
      RISK_TAGS="$2"
      shift 2
      ;;
    --contract)
      CONTRACT_FILE="$2"
      shift 2
      ;;
    --help|-h)
      usage
      exit 0
      ;;
    *)
      echo "Unknown option: $1" >&2
      usage
      exit 1
      ;;
  esac
done

# Generate tests
generate_tests "$DIFF_FILE" "$COVERAGE_FILE" "$RISK_TAGS" "$CONTRACT_FILE"
